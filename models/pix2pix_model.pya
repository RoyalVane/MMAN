import numpy as np
import torch
import os
import random
import torch.nn as nn
from torchvision import models
from collections import OrderedDict
from torch.autograd import Variable
import util.util as util
from util.image_pool import ImagePool
from .base_model import BaseModel
from . import networks
from . import pose_generator
yGrad = torch.zeros((1, 1))

def extract(xVar):
	global yGrad
	yGrad = xVar

Scale_small = 0.75
Scale_large = 1.25
Attribute_scale = 8

def swap(T, m, n): #Distinguish left & right
    A = T.data.cpu().numpy()
    A[:, [m, n], :, :] = A[:, [n, m], :, :]
    return Variable(torch.from_numpy(A)).cuda()

class Pix2PixModel(BaseModel):
    def name(self):
        return 'Pix2PixModel'

    def initialize(self, opt):
        BaseModel.initialize(self, opt)
        self.epoch = 1
        self.isTrain = opt.isTrain
        # define tensors for G1
        self.input_A = self.Tensor(opt.batchSize, opt.input_nc,
                                   opt.fineSize, opt.fineSize)
        self.input_A_S = self.Tensor(opt.batchSize, opt.input_nc,
                                   int(opt.fineSize * Scale_small), int(opt.fineSize * Scale_small))
        self.input_A_L = self.Tensor(opt.batchSize, opt.input_nc,
                                   int(opt.fineSize * Scale_large), int(opt.fineSize * Scale_large))
        
        self.input_A_Attribute = self.Tensor(opt.batchSize, opt.input_nc, int(opt.fineSize/Attribute_scale), int(opt.fineSize/Attribute_scale))
        
        self.input_B_GAN = self.Tensor(opt.batchSize, opt.output_nc,
                                   opt.fineSize, opt.fineSize)
        self.input_B_L1 = self.LongTensor(opt.batchSize,
                                   opt.fineSize, opt.fineSize)
        
        self.input_B_Attribute_GAN = self.Tensor(opt.batchSize, opt.output_nc, 
                                    int(opt.fineSize/Attribute_scale), int(opt.fineSize/Attribute_scale))
        
        self.input_B_Attribute_L1 = self.LongTensor(opt.batchSize, 
                                    int(opt.fineSize/Attribute_scale), int(opt.fineSize/Attribute_scale))
        

        
        #define hook
        self.hook = networks.UnetHook()
        
        # load/define networks
        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf,
                                          opt.which_model_netG, self.hook, opt.fineSize, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids)
        
        self.netA = networks.define_A(opt.output_nc * 3, 512, opt.init_type, self.gpu_ids)
        
        if self.isTrain:
            use_sigmoid = opt.no_lsgan

            
            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf,
                                              opt.which_model_netD,
                                              opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids)
    
            self.netD2 = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf,
                                              opt.which_model_netD,
                                              opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids)
            
        if not self.isTrain or opt.continue_train:
            self.load_network(self.netG, 'G', opt.which_epoch)
            #self.load_network(self.netA, 'A', opt.which_epoch)
            if self.isTrain:
                self.load_network(self.netD, 'D', opt.which_epoch)
                self.load_network(self.netD2, 'D2', opt.which_epoch)
                    

        if self.isTrain:
            self.fake_AB_pool = ImagePool(opt.pool_size)
            self.old_lr = opt.lr
            # define loss functions
            self.loss_weight = self.Tensor([1.0, 1.1, 1.2, 1.3, 1.4, 1.3, 1.4])
            self.criterionL1 = torch.nn.NLLLoss2d(weight=self.loss_weight)
            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)
            self.criterionAttributeL1 = torch.nn.NLLLoss2d(weight=self.loss_weight)
            self.criterionAttributeGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)
            self.criterionA = torch.nn.NLLLoss2d(ignore_index = 0)
            
            # initialize optimizers
            self.schedulers = []
            self.optimizers = []
            
            ignored_params = list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[3].parameters() )) \
            + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[5].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[6].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[23].con0.parameters() )) \
            + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[25].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[4].U1[26].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[4].con1.parameters() )) \
            + list(map(id, self.netG.model.U4[2].U3[4].U2[6].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].U2[7].parameters() )) + list(map(id, self.netG.model.U4[2].U3[4].con2.parameters() )) \
            + list(map(id, self.netG.model.U4[2].U3[6].parameters() )) + list(map(id, self.netG.model.U4[2].U3[7].parameters() )) + list(map(id, self.netG.model.U4[2].con3.parameters() )) \
            + list(map(id, self.netG.model.U4[4].parameters() ))
            base_params = filter(lambda p: id(p) not in ignored_params, self.netG.parameters())
            self.optimizer_G = torch.optim.Adam([{'params': base_params, 'lr': 0.1 * opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[3].parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[5].parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[23].U0[6].parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[23].con0.parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[25].parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].U2[4].U1[26].parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].U2[4].con1.parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].U2[6].parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].U2[7].parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[4].con2.parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[6].parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].U3[7].parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[2].con3.parameters(), 'lr': opt.lr},
                                                {'params': self.netG.model.U4[4].parameters(), 'lr': opt.lr},
                                                ], betas=(opt.beta1, 0.999), weight_decay = 1e-4)
            
    
# =============================================================================
#             self.optimizer_G = torch.optim.Adam(self.netG.parameters(),
#                                                 lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay = 1e-4)
# =============================================================================
            
            self.optimizer_A = torch.optim.Adam(self.netA.parameters(), 
                                                lr=opt.lr/10, betas=(opt.beta1, 0.999))
    
            self.optimizer_D = torch.optim.Adam(self.netD.parameters(),
                                                lr=opt.lr * 5, betas=(opt.beta1, 0.999), weight_decay = 1e-4)
            
            self.optimizer_D2 = torch.optim.Adam(self.netD2.parameters(),
                                                lr=opt.lr * 5, betas=(opt.beta1, 0.999), weight_decay = 1e-4)
            

            
            self.optimizers.append(self.optimizer_G)
            self.optimizers.append(self.optimizer_A)
            self.optimizers.append(self.optimizer_D)
            self.optimizers.append(self.optimizer_D2)
            
            for optimizer in self.optimizers:
                self.schedulers.append(networks.get_scheduler(optimizer, opt))

        print('---------- Networks initialized -------------')
        networks.print_network(self.netG)
        networks.print_network(self.netA)
        if self.isTrain:
            networks.print_network(self.netD)
            networks.print_network(self.netD2)
        print('-----------------------------------------------')

# =============================================================================
#     input_A: original color image
#     input_B_GAN: original label image (multiple channels) for GAN loss calculation
#     input_B_L1: original label image (single channel) for L1 loss calculation
#     input_B_Attribute: original thumbnail for Attribute loss calculation
# =============================================================================
    def set_input(self, input):
        AtoB = self.opt.which_direction == 'AtoB'
        self.image_paths = input['A_paths' if AtoB else 'B_paths']
        
        #image
        input_A = input['A']
        input_A_S = input['A_S']
        input_A_L = input['A_L']
        input_A_Attribute = input['A_Attribute']
        
        #label
        input_B_GAN = input['B_GAN']
        input_B_L1 = input['B_L1']
        input_B_Attribute_L1 = input['B_Attribute_L1']
        input_B_Attribute_GAN = input['B_Attribute_GAN']
        
        #G1
        self.input_A.resize_(input_A.size()).copy_(input_A)
        self.input_A_S.resize_(input_A_S.size()).copy_(input_A_S)
        self.input_A_L.resize_(input_A_L.size()).copy_(input_A_L)
        self.input_A_Attribute.resize_(input_A_Attribute.size()).copy_(input_A_Attribute)
        
        self.input_B_GAN.resize_(input_B_GAN.size()).copy_(input_B_GAN)
        self.input_B_L1.resize_(input_B_L1.size()).copy_(input_B_L1)
        self.input_B_Attribute_GAN.resize_(input_B_Attribute_GAN.size()).copy_(input_B_Attribute_GAN)
        self.input_B_Attribute_L1.resize_(input_B_Attribute_L1.size()).copy_(input_B_Attribute_L1)
        
    def forward(self):
        self.real_A = Variable(self.input_A)
        self.real_A_Attribute = Variable(self.input_A_Attribute)
        self.real_A_S = Variable(self.input_A_S)
        self.real_A_L = Variable(self.input_A_L)

        #Copy from files
        self.real_B_GAN = Variable(self.input_B_GAN) #multi-channel target for label map
        self.real_B_L1 = Variable(self.input_B_L1) #single-channel target for label map
        self.real_B_Attribute_GAN = Variable(self.input_B_Attribute_GAN) #multi-channel target for thumbnail
        self.real_B_Attribute_L1 = Variable(self.input_B_Attribute_L1) # single-channel target for thumbnail
        
        #Generate from networks
        self.fake_B_GAN_M = self.netG(self.real_A)['GAN'] #multi-channel label map--> target real_B_GAN
        self.fake_B_L1_M = self.netG(self.real_A)['L1'] #multi-channel label map but nagtive --> target real_B_L1
        self.fake_B_Attribute_GAN_M = self.hook.get_value()['GAN'] #multi-channel thumbnail --> real_B_Attribute_GAN
        self.fake_B_Attribute_L1_M = self.hook.get_value()['L1'] #multi-channel thumbnail but nagtive --> real_B_Attribute_L1
        
        self.fake_B_GAN_S = self.netG(self.real_A_S)['GAN']
        self.fake_B_L1_S = self.netG(self.real_A_S)['L1']
        self.fake_B_Attribute_GAN_S = self.hook.get_value()['GAN'] #multi-channel thumbnail --> real_B_Attribute_GAN
        self.fake_B_Attribute_L1_S = self.hook.get_value()['L1'] #multi-channel thumbnail but nagtive --> real_B_Attribute_L1
        
        self.fake_B_GAN_L = self.netG(self.real_A_L)['GAN']
        self.fake_B_L1_L = self.netG(self.real_A_L)['L1']
        self.fake_B_Attribute_GAN_L = self.hook.get_value()['GAN'] #multi-channel thumbnail --> real_B_Attribute_GAN
        self.fake_B_Attribute_L1_L = self.hook.get_value()['L1'] #multi-channel thumbnail but nagtive --> real_B_Attribute_L1
        
        self.fake_B_Attribute_GAN_MSL = (self.fake_B_Attribute_GAN_M + self.fake_B_Attribute_GAN_S + self.fake_B_Attribute_GAN_L) / 3
        self.fake_B_GAN_MSL = (self.fake_B_GAN_M + self.fake_B_GAN_S + self.fake_B_GAN_L) / 3
        
    # no backprop gradients
    def test(self):
        self.real_A = Variable(self.input_A, volatile=True)
        self.real_A_S = Variable(self.input_A_S, volatile=True)
        self.real_A_L = Variable(self.input_A_L, volatile=True)

        M = nn.Upsample((int(self.real_A.size(2)), int(self.real_A.size(3))), mode='bilinear')

        self.fake_B_L1_M = self.netG(self.real_A)['L1']
        self.fake_B_L1_S = self.netG(self.real_A_S)['L1']
        self.fake_B_L1_L = self.netG(self.real_A_L)['L1']
        
        self.fake_B_L1_M = M(self.fake_B_L1_M)
        self.fake_B_L1_S = M(self.fake_B_L1_S)
        self.fake_B_L1_L = M(self.fake_B_L1_L)
        
        
        self.fake_B_L1_MSL = torch.cat((self.fake_B_L1_M, self.fake_B_L1_S, self.fake_B_L1_L), 1)
        self.Attention = self.netA(self.fake_B_L1_MSL)

        self.fake_B_L1_MSL2 = self.fake_B_L1_M * self.Attention[:, 0, :, :] + self.fake_B_L1_S * self.Attention[:, 1, :, :] + self.fake_B_L1_L * self.Attention[:, 2, :, :] 
        self.real_B_GAN = Variable(self.input_B_GAN, volatile=True)

    # get image paths
    def get_image_paths(self):
        return self.image_paths

    def backward_D(self):
        # Fake
        # stop backprop to the generator by detaching fake_B
        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B_GAN_MSL), 1).data)
        pred_fake = self.netD(fake_AB.detach())
        self.loss_D_fake = self.criterionGAN(pred_fake, False)
        
        # Real
        real_AB = torch.cat((self.real_A, self.real_B_GAN), 1)
        pred_real = self.netD(real_AB)
        self.loss_D_real = self.criterionGAN(pred_real, True)
        
        # Combined loss
        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5
        self.loss_D.backward()

    
    def backward_D2(self):
        fake_AB_Attribute = torch.cat((self.real_A_Attribute, self.fake_B_Attribute_GAN_MSL), 1)
        pred_fake_Attribute = self.netD2(fake_AB_Attribute.detach())
        self.loss_D_fake_Attribute = self.criterionAttributeGAN(pred_fake_Attribute, False)
        
        real_AB_Attribute = torch.cat((self.real_A_Attribute, self.real_B_Attribute_GAN), 1)
        pred_real_Attribute = self.netD2(real_AB_Attribute)
        self.loss_D_real_Attribute = self.criterionAttributeGAN(pred_real_Attribute, True)
        
        self.loss_D_Attribute = (self.loss_D_fake_Attribute + self.loss_D_real_Attribute) * 0.5
        self.loss_D_Attribute.backward()
        
    
    def backward_G_M(self):
        #GAN loss: G(x) should fake the discriminator
        fake_AB = torch.cat((self.real_A, self.fake_B_GAN_MSL), 1)
        pred_fake = self.netD(fake_AB.detach())
        self.loss_G_GAN = self.criterionGAN(pred_fake, True)
        
        #Attribute GAN loss: G(x) should fake the discriminator2
        fake_AB_Attribute = torch.cat((self.real_A_Attribute, self.fake_B_Attribute_GAN_MSL), 1)
        pred_fake_Attribute = self.netD2(fake_AB_Attribute.detach())
        self.loss_G_GAN_Attribute = self.criterionAttributeGAN(pred_fake_Attribute, True)
        
        #L1 loss: Minimize logSoftmax concats NLL2d between original size
        self.loss_G_L1_M = self.criterionL1(self.fake_B_L1_M, self.real_B_L1) * self.opt.lambda_A
        
        #Attribute L1 loss: Minimize logSoftmax concats NLL2d between thumbnail
        self.loss_G_L1_Attribute_M = self.criterionAttributeL1(self.fake_B_Attribute_L1_M, self.real_B_Attribute_L1) * self.opt.lambda_A
        
        #Total loss
        self.loss_G_M = 8.0 * self.loss_G_L1_M + 1.0 * self.loss_G_L1_Attribute_M +  \
            0.2 * self.loss_G_GAN + 0.2 * self.loss_G_GAN_Attribute #for Pascal
        
        self.loss_G_M.backward(retain_graph = True)
        
    def backward_G_S(self):
        #GAN loss: G(x) should fake the discriminator
        fake_AB = torch.cat((self.real_A, self.fake_B_GAN_MSL), 1)
        pred_fake = self.netD(fake_AB.detach())
        self.loss_G_GAN = self.criterionGAN(pred_fake, True)
        
        #Attribute GAN loss: G(x) should fake the discriminator2
        fake_AB_Attribute = torch.cat((self.real_A_Attribute, self.fake_B_Attribute_GAN_MSL), 1)
        pred_fake_Attribute = self.netD2(fake_AB_Attribute.detach())
        self.loss_G_GAN_Attribute = self.criterionAttributeGAN(pred_fake_Attribute, True)
        
        #L1 loss: Minimize logSoftmax concats NLL2d between original size
        self.loss_G_L1_S = self.criterionL1(self.fake_B_L1_S, self.real_B_L1) * self.opt.lambda_A
        
        #Attribute L1 loss: Minimize logSoftmax concats NLL2d between thumbnail
        self.loss_G_L1_Attribute_S = self.criterionAttributeL1(self.fake_B_Attribute_L1_S, self.real_B_Attribute_L1) * self.opt.lambda_A
        
        self.loss_G_S = 8.0 * self.loss_G_L1_S + 1.0 * self.loss_G_L1_Attribute_S +  \
            0.2 * self.loss_G_GAN + 0.2 * self.loss_G_GAN_Attribute #for Pascal
        
        self.loss_G_S.backward(retain_graph = True)
        
    def backward_G_L(self):
        #GAN loss: G(x) should fake the discriminator
        fake_AB = torch.cat((self.real_A, self.fake_B_GAN_MSL), 1)
        pred_fake = self.netD(fake_AB.detach())
        self.loss_G_GAN = self.criterionGAN(pred_fake, True)
        
        #Attribute GAN loss: G(x) should fake the discriminator2
        fake_AB_Attribute = torch.cat((self.real_A_Attribute, self.fake_B_Attribute_GAN_MSL), 1)
        pred_fake_Attribute = self.netD2(fake_AB_Attribute.detach())
        self.loss_G_GAN_Attribute = self.criterionAttributeGAN(pred_fake_Attribute, True)
        
        #L1 loss: Minimize logSoftmax concats NLL2d between original size
        self.loss_G_L1_L = self.criterionL1(self.fake_B_L1_L, self.real_B_L1) * self.opt.lambda_A
        
        #Attribute L1 loss: Minimize logSoftmax concats NLL2d between thumbnail
        self.loss_G_L1_Attribute_L = self.criterionAttributeL1(self.fake_B_Attribute_L1_L, self.real_B_Attribute_L1) * self.opt.lambda_A
        
        self.loss_G_L = 8.0 * self.loss_G_L1_L + 1.0 * self.loss_G_L1_Attribute_L +  \
            0.2 * self.loss_G_GAN + 0.2 * self.loss_G_GAN_Attribute #for Pascal
        
        self.loss_G_L.backward(retain_graph = True)
        
    def backward_A(self):
        self.fake_B_L1_MSL = torch.cat((self.fake_B_L1_M, self.fake_B_L1_S, self.fake_B_L1_L), 1)
        self.Attention = self.netA(self.fake_B_L1_MSL)
        self.fake_B_L1_MSL2 = torch.mul(self.fake_B_L1_M, self.Attention[:, 0, :, :]) + torch.mul(self.fake_B_L1_S, self.Attention[:, 1, :, :]) + torch.mul(self.fake_B_L1_L, self.Attention[:, 2, :, :])
        self.loss_A = self.criterionA(self.fake_B_L1_MSL2, self.real_B_L1)
        self.loss_A.backward(retain_graph = True)
        
    def backward_MSL(self):
        self.loss_MSL = self.criterionL1(self.fake_B_L1_MSL2, self.real_B_L1) * self.opt.lambda_A * 3
        self.loss_MSL.backward()
        
    def optimize_parameters(self):
        self.forward()

        self.optimizer_D.zero_grad()
        self.backward_D()
        self.optimizer_D.step()
        
        self.optimizer_D2.zero_grad()
        self.backward_D2()
        self.optimizer_D2.step()
        
        #k = random.random()
        
        #if k < 1/3:
        self.optimizer_G.zero_grad()
        self.backward_G_M()
        self.optimizer_G.step()

        #elif 1/3 < k < 2/3:
        self.optimizer_G.zero_grad()
        self.backward_G_S()
        self.optimizer_G.step()
            
        #else:
        self.optimizer_G.zero_grad()
        self.backward_G_L()
        self.optimizer_G.step()
            
        self.optimizer_A.zero_grad()
        self.backward_A()
        self.optimizer_A.step()
            
        #if self.epoch > 10:
        self.optimizer_G.zero_grad()
        self.backward_MSL()
        self.optimizer_G.step()
        
    def get_current_errors(self):

        return OrderedDict([('G_GAN', self.loss_G_GAN.data[0]),
                            ('G_L1', self.loss_G_L1_M.data[0]),
                            ('D_real', self.loss_D_real.data[0]),
                            ('D_fake', self.loss_D_fake.data[0]),
                            ('G_GAN_Attri', self.loss_G_GAN_Attribute.data[0]),
                            ('G_L1_Attri', self.loss_G_L1_Attribute_M.data[0]),
                            ('D_real_Attri', self.loss_D_real_Attribute.data[0]),
                            ('D_fake_Attri', self.loss_D_fake_Attribute.data[0])
                            ])

    def get_current_visuals(self):
        real_A = util.tensor2im(self.real_A.data)
        #fake_B_M = util.ndim_tensor2im(self.fake_B_L1_M.data, dataset = self.opt.dataset)
        #fake_B_S = util.ndim_tensor2im(self.fake_B_L1_S.data, dataset = self.opt.dataset)
        #fake_B_L = util.ndim_tensor2im(self.fake_B_L1_L.data, dataset = self.opt.dataset)
        fake_B = util.ndim_tensor2im(self.fake_B_L1_MSL2.data, dataset = self.opt.dataset)
        real_B = util.ndim_tensor2im(self.real_B_GAN.data, dataset = self.opt.dataset)
        #Attention1 = util.onedim_tensor2im(self.Attention[:, 0, :, :], dataset = self.opt.dataset)
        #Attention2 = util.onedim_tensor2im(self.Attention[:, 1, :, :], dataset = self.opt.dataset)
        #Attention3 = util.onedim_tensor2im(self.Attention[:, 2, :, :], dataset = self.opt.dataset)
        #return OrderedDict([('real_A', real_A), ('fake_B_M', fake_B_M), ('fake_B_S', fake_B_S), ('fake_B_L', fake_B_L), 
                           # ('real_B', real_B), ('fake_B', fake_B),
                           # ('Attention1', Attention1), ('Attention2', Attention2), ('Attention3', Attention3)])
        return OrderedDict([('real_A', real_A), ('real_B', real_B), ('fake_B', fake_B)])

    def save(self, label):
        self.epoch = self.epoch + 1
        print('weight L1:', min(self.epoch / 20.0, 5.0) * self.opt.lambda_A, 'weight L1 Attri:', self.opt.lambda_A, 
              'weight GAN:', min(self.epoch // 20.0, 1.0), 'weight GAN Attri:', 0.5)
        self.save_network(self.netG, 'G', label, self.gpu_ids)
        self.save_network(self.netA, 'A', label, self.gpu_ids)
        self.save_network(self.netD, 'D', label, self.gpu_ids)
        self.save_network(self.netD2, 'D2', label, self.gpu_ids)
